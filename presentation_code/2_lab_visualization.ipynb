{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set style for better-looking plots\nsns.set_style(\"whitegrid\")\nplt.rcParams['figure.figsize'] = (12, 8)\nplt.rcParams['font.size'] = 12\n\n# Create error values (difference between prediction and actual)\nerrors = np.linspace(-5, 5, 100)\n\n# Calculate loss functions for INDIVIDUAL errors (not averaged)\nmse_per_point = errors ** 2  # Squared error per point\nmae_per_point = np.abs(errors)  # Absolute error per point\n\n# RMSE is typically calculated as sqrt(MEAN(squared errors))\n# But to show the difference, let's show the gradient/penalty curve\n# This shows how much each error contributes to the loss\n\n# Create the comparison plot\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Plot 1: All three loss functions together (individual error contributions)\nax1 = axes[0, 0]\nax1.plot(errors, mse_per_point, label='MSE (Squared Error)', linewidth=2.5, color='#e74c3c')\nax1.plot(errors, mae_per_point, label='MAE (Absolute Error)', linewidth=2.5, color='#2ecc71')\n# For RMSE visualization, show sqrt of squared error (which equals |error|)\n# Instead, let's show the derivative to demonstrate difference in gradient\nax1.plot(errors, np.abs(errors) * 1.5, label='RMSE penalty (scaled)', linewidth=2.5, \n         color='#3498db', linestyle='--')\nax1.set_xlabel('Error (Prediction - Actual)', fontsize=12)\nax1.set_ylabel('Loss Contribution', fontsize=12)\nax1.set_title('Loss Functions: Error Contribution Comparison', fontsize=14, fontweight='bold')\nax1.legend(fontsize=11)\nax1.grid(True, alpha=0.3)\n\n# Plot 2: Gradient comparison (how loss changes with error)\nax2 = axes[0, 1]\n# Gradient of MSE is 2*error, gradient of MAE is sign(error)\nmse_gradient = 2 * errors\nmae_gradient = np.sign(errors)\nax2.plot(errors, mse_gradient, linewidth=3, color='#e74c3c', label='MSE gradient')\nax2.plot(errors, mae_gradient, linewidth=3, color='#2ecc71', label='MAE gradient')\nax2.set_xlabel('Error', fontsize=12)\nax2.set_ylabel('Gradient (penalty rate)', fontsize=12)\nax2.set_title('Loss Function Gradients', fontsize=14, fontweight='bold')\nax2.legend(fontsize=11)\nax2.grid(True, alpha=0.3)\nax2.text(0, ax2.get_ylim()[1]*0.7, 'MSE gradient increases with error\\nMAE gradient is constant', \n         ha='center', fontsize=10, style='italic', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n\n# Plot 3: Actual comparison with sample data\nax3 = axes[1, 0]\n# Generate sample predictions with different error patterns\nnp.random.seed(42)\nscenario_labels = ['Few large\\nerrors', 'Many small\\nerrors', 'Mixed\\nerrors']\nmse_vals = []\nrmse_vals = []\nmae_vals = []\n\n# Scenario 1: Few large errors\nerrors1 = np.concatenate([np.random.normal(0, 0.5, 40), np.random.normal(0, 3, 10)])\nmse_vals.append(np.mean(errors1 ** 2))\nrmse_vals.append(np.sqrt(np.mean(errors1 ** 2)))\nmae_vals.append(np.mean(np.abs(errors1)))\n\n# Scenario 2: Many small errors\nerrors2 = np.random.normal(0, 0.8, 50)\nmse_vals.append(np.mean(errors2 ** 2))\nrmse_vals.append(np.sqrt(np.mean(errors2 ** 2)))\nmae_vals.append(np.mean(np.abs(errors2)))\n\n# Scenario 3: Mixed\nerrors3 = np.random.normal(0, 1.5, 50)\nmse_vals.append(np.mean(errors3 ** 2))\nrmse_vals.append(np.sqrt(np.mean(errors3 ** 2)))\nmae_vals.append(np.mean(np.abs(errors3)))\n\nx = np.arange(len(scenario_labels))\nwidth = 0.25\nax3.bar(x - width, mse_vals, width, label='MSE', color='#e74c3c', alpha=0.8)\nax3.bar(x, rmse_vals, width, label='RMSE', color='#3498db', alpha=0.8)\nax3.bar(x + width, mae_vals, width, label='MAE', color='#2ecc71', alpha=0.8)\nax3.set_xlabel('Error Pattern', fontsize=12)\nax3.set_ylabel('Loss Value', fontsize=12)\nax3.set_title('Loss Metrics on Different Error Patterns', fontsize=14, fontweight='bold')\nax3.set_xticks(x)\nax3.set_xticklabels(scenario_labels)\nax3.legend(fontsize=11)\nax3.grid(True, alpha=0.3, axis='y')\n\n# Plot 4: Key insight - showing why RMSE differs from MAE\nax4 = axes[1, 1]\n# Show cumulative effect\nsample_sizes = np.arange(1, 51)\ncumulative_rmse = []\ncumulative_mae = []\n\ntest_errors = np.random.normal(0, 2, 50)\nfor n in sample_sizes:\n    cumulative_rmse.append(np.sqrt(np.mean(test_errors[:n] ** 2)))\n    cumulative_mae.append(np.mean(np.abs(test_errors[:n])))\n\nax4.plot(sample_sizes, cumulative_rmse, linewidth=3, color='#3498db', label='RMSE')\nax4.plot(sample_sizes, cumulative_mae, linewidth=3, color='#2ecc71', label='MAE')\nax4.set_xlabel('Number of samples', fontsize=12)\nax4.set_ylabel('Loss Value', fontsize=12)\nax4.set_title('RMSE vs MAE: Cumulative Behavior', fontsize=14, fontweight='bold')\nax4.legend(fontsize=11)\nax4.grid(True, alpha=0.3)\nax4.text(25, ax4.get_ylim()[1]*0.8, 'RMSE > MAE when large errors exist', \n         ha='center', fontsize=10, style='italic', bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.3))\n\nplt.tight_layout()\nplt.savefig('loss_functions_comparison.png', dpi=300, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "source": "# Loss Functions Comparison: MSE, RMSE, and MAE\n\nThis presentation notebook provides comprehensive visualizations comparing three fundamental regression loss functions.\n\n## Comprehensive Visualization: Multiple Perspectives\n\nThis visualization shows:\n1. Error contribution curves for different loss functions\n2. Gradient comparison (how quickly loss changes with error)\n3. Performance on different error patterns\n4. Cumulative behavior over samples",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Simplified Presentation View\n\nA clean, focused visualization showing how **individual errors** are penalized differently by MSE vs MAE.\n\n**Key insight:** MSE squares errors, so large errors contribute exponentially more to the loss.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Summary: When to Use Each Loss Function\n\n### MSE (Mean Squared Error)\n- **Formula**: $\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$\n- **Characteristics**: Squares the errors, heavily penalizing large errors\n- **When to use**: When large errors are particularly undesirable (e.g., safety-critical applications)\n- **Disadvantage**: Not in the same units as the original data (squared units)\n\n### RMSE (Root Mean Squared Error)\n- **Formula**: $\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}$\n- **Characteristics**: Square root of MSE, returns error to original scale\n- **When to use**: When you want MSE's outlier sensitivity but in interpretable units\n- **Advantage**: Same units as the target variable, easier to interpret\n\n### MAE (Mean Absolute Error)\n- **Formula**: $\\frac{1}{n}\\sum_{i=1}^{n}|y_i - \\hat{y}_i|$\n- **Characteristics**: Linear penalty for all errors\n- **When to use**: When all errors should be weighted equally, or when outliers shouldn't dominate\n- **Advantage**: More robust to outliers than MSE/RMSE\n\n### Key Takeaway\n**RMSE ‚â• MAE** always, with equality only when all errors have the same magnitude. The gap between them indicates the presence of large errors/outliers.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Simple clean plot for presentation - showing individual error contributions\nfig, ax = plt.subplots(figsize=(12, 7))\n\nerrors_range = np.linspace(-5, 5, 100)\n\n# Plot the error contribution curves (for INDIVIDUAL errors, not averaged)\nsns.lineplot(x=errors_range, y=errors_range**2, label='MSE: Squared Error (e¬≤)', \n             linewidth=3, ax=ax, color='#e74c3c')\nsns.lineplot(x=errors_range, y=np.abs(errors_range), label='MAE: Absolute Error (|e|)', \n             linewidth=3, ax=ax, color='#2ecc71')\n\nax.set_xlabel('Error (Prediction - Actual)', fontsize=14, fontweight='bold')\nax.set_ylabel('Loss Contribution per Sample', fontsize=14, fontweight='bold')\nax.set_title('MSE vs MAE: How Individual Errors are Penalized', fontsize=16, fontweight='bold', pad=20)\nax.legend(fontsize=12, loc='upper center')\nax.axvline(x=0, color='black', linestyle='--', alpha=0.3, linewidth=1)\nax.axhline(y=0, color='black', linestyle='--', alpha=0.3, linewidth=1)\n\n# Add annotations\nax.annotate('Large errors penalized\\nmuch more heavily', xy=(3, 9), xytext=(1.5, 15),\n            arrowprops=dict(arrowstyle='->', color='#e74c3c', lw=2),\n            fontsize=11, color='#e74c3c', fontweight='bold',\n            bbox=dict(boxstyle='round', facecolor='white', edgecolor='#e74c3c', alpha=0.8))\n\nax.annotate('Linear penalty\\nfor all errors', xy=(3, 3), xytext=(3.5, 7),\n            arrowprops=dict(arrowstyle='->', color='#2ecc71', lw=2),\n            fontsize=11, color='#2ecc71', fontweight='bold',\n            bbox=dict(boxstyle='round', facecolor='white', edgecolor='#2ecc71', alpha=0.8))\n\n# Add text box explaining RMSE\ntextstr = 'Note: RMSE = ‚àö(MSE averaged over all samples)\\n' \\\n          'RMSE always falls between MAE and MSE values\\n' \\\n          'See cell 3 for RMSE comparison!'\nprops = dict(boxstyle='round', facecolor='lightblue', alpha=0.3)\nax.text(0.02, 0.98, textstr, transform=ax.transAxes, fontsize=10,\n        verticalalignment='top', bbox=props, style='italic')\n\nplt.tight_layout()\nplt.savefig('loss_functions_simple.png', dpi=300, bbox_inches='tight')\nplt.show()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Why only MSE and MAE are shown on this plot:\")\nprint(\"=\"*60)\nprint(\"‚Ä¢ This plot shows loss contribution for INDIVIDUAL errors\")\nprint(\"‚Ä¢ For a single error e:\")\nprint(\"  - MSE contribution = e¬≤\")\nprint(\"  - MAE contribution = |e|\")\nprint(\"  - RMSE is only meaningful when averaged over multiple errors!\")\nprint(\"\\n‚Ä¢ RMSE = sqrt(mean(e¬≤)), so it requires averaging first\")\nprint(\"‚Ä¢ See Cell 3 for RMSE vs MAE comparison with real data!\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Demonstration: Why RMSE ‚â† MAE (the key difference for students!)\nimport pandas as pd\n\nprint(\"=\" * 70)\nprint(\"KEY CONCEPT: RMSE vs MAE difference appears when AVERAGING errors\")\nprint(\"=\" * 70)\n\n# Example 1: Set of predictions with different error patterns\nprint(\"\\nüìä Example 1: Two models with different error patterns\\n\")\n\n# Model A: Consistent small errors\nerrors_model_a = np.array([1, 1, 1, 1, 1])\nprint(f\"Model A errors: {errors_model_a}\")\nprint(f\"  MAE  = mean(|errors|) = {np.mean(np.abs(errors_model_a)):.3f}\")\nprint(f\"  MSE  = mean(errors¬≤)  = {np.mean(errors_model_a**2):.3f}\")\nprint(f\"  RMSE = sqrt(MSE)      = {np.sqrt(np.mean(errors_model_a**2)):.3f}\")\n\n# Model B: One large error, rest small\nerrors_model_b = np.array([0, 0, 0, 0, 5])\nprint(f\"\\nModel B errors: {errors_model_b}\")\nprint(f\"  MAE  = mean(|errors|) = {np.mean(np.abs(errors_model_b)):.3f}\")\nprint(f\"  MSE  = mean(errors¬≤)  = {np.mean(errors_model_b**2):.3f}\")\nprint(f\"  RMSE = sqrt(MSE)      = {np.sqrt(np.mean(errors_model_b**2)):.3f}\")\n\nprint(\"\\nüí° Notice: Both have same MAE (1.0), but RMSE is much higher for Model B!\")\nprint(\"   This is because RMSE penalizes the large error (5) much more heavily.\")\n\n# Create visualization\nfig, axes = plt.subplots(1, 3, figsize=(16, 5))\n\n# Plot 1: Show the errors\nax1 = axes[0]\nx_pos = np.arange(5)\nwidth = 0.35\nax1.bar(x_pos - width/2, errors_model_a, width, label='Model A', color='#3498db', alpha=0.8)\nax1.bar(x_pos + width/2, errors_model_b, width, label='Model B', color='#e74c3c', alpha=0.8)\nax1.set_xlabel('Prediction Number', fontsize=12)\nax1.set_ylabel('Error Value', fontsize=12)\nax1.set_title('Error Distribution', fontsize=14, fontweight='bold')\nax1.legend()\nax1.grid(True, alpha=0.3, axis='y')\n\n# Plot 2: Show the metrics comparison\nax2 = axes[1]\nmetrics = ['MAE', 'RMSE', 'MSE']\nmodel_a_metrics = [\n    np.mean(np.abs(errors_model_a)),\n    np.sqrt(np.mean(errors_model_a**2)),\n    np.mean(errors_model_a**2)\n]\nmodel_b_metrics = [\n    np.mean(np.abs(errors_model_b)),\n    np.sqrt(np.mean(errors_model_b**2)),\n    np.mean(errors_model_b**2)\n]\n\nx_pos = np.arange(len(metrics))\nwidth = 0.35\nbars1 = ax2.bar(x_pos - width/2, model_a_metrics, width, label='Model A (consistent)', \n                color='#3498db', alpha=0.8)\nbars2 = ax2.bar(x_pos + width/2, model_b_metrics, width, label='Model B (1 outlier)', \n                color='#e74c3c', alpha=0.8)\n\n# Add value labels on bars\nfor bars in [bars1, bars2]:\n    for bar in bars:\n        height = bar.get_height()\n        ax2.text(bar.get_x() + bar.get_width()/2., height,\n                f'{height:.2f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n\nax2.set_ylabel('Loss Value', fontsize=12)\nax2.set_title('Loss Metrics Comparison', fontsize=14, fontweight='bold')\nax2.set_xticks(x_pos)\nax2.set_xticklabels(metrics)\nax2.legend()\nax2.grid(True, alpha=0.3, axis='y')\n\n# Plot 3: Mathematical explanation\nax3 = axes[2]\nax3.axis('off')\nexplanation = \"\"\"\nWhy RMSE ‚â† MAE:\n\nMAE Formula:\nMAE = (|e‚ÇÅ| + |e‚ÇÇ| + ... + |e‚Çô|) / n\n\nRMSE Formula:\nRMSE = ‚àö[(e‚ÇÅ¬≤ + e‚ÇÇ¬≤ + ... + e‚Çô¬≤) / n]\n\nKey Difference:\n‚Ä¢ Squaring before averaging emphasizes \n  larger errors\n‚Ä¢ Square root brings back to original \n  scale, but the emphasis remains\n\nModel A: (1¬≤+1¬≤+1¬≤+1¬≤+1¬≤)/5 = 1.0\n         ‚àö1.0 = 1.0\n\nModel B: (0¬≤+0¬≤+0¬≤+0¬≤+5¬≤)/5 = 5.0\n         ‚àö5.0 = 2.236\n\nThe large error (5) dominates RMSE!\n\"\"\"\nax3.text(0.1, 0.5, explanation, fontsize=11, family='monospace',\n         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),\n         verticalalignment='center')\n\nplt.tight_layout()\nplt.savefig('rmse_vs_mae_explained.png', dpi=300, bbox_inches='tight')\nplt.show()\n\n# Additional example with realistic prediction errors\nprint(\"\\n\" + \"=\" * 70)\nprint(\"üìä Example 2: Realistic prediction scenario\")\nprint(\"=\" * 70)\n\nnp.random.seed(42)\n# Create realistic errors\nnormal_errors = np.random.normal(0, 1, 20)\nerrors_with_outliers = np.concatenate([np.random.normal(0, 1, 18), np.array([5, -6])])\n\ncomparison_df = pd.DataFrame({\n    'Dataset': ['Normal errors', 'With 2 outliers'],\n    'MAE': [np.mean(np.abs(normal_errors)), np.mean(np.abs(errors_with_outliers))],\n    'RMSE': [np.sqrt(np.mean(normal_errors**2)), np.sqrt(np.mean(errors_with_outliers**2))],\n    'MSE': [np.mean(normal_errors**2), np.mean(errors_with_outliers**2)]\n})\n\nprint(comparison_df.to_string(index=False))\nprint(\"\\nüí° RMSE increases more than MAE when outliers are present!\")\nprint(\"   This makes RMSE more sensitive to large errors.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Differences for Students:\n",
    "\n",
    "### MSE (Mean Squared Error)\n",
    "- **Formula**: $\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$\n",
    "- **Characteristics**: Squares the errors, heavily penalizing large errors\n",
    "- **When to use**: When large errors are particularly undesirable\n",
    "- **Disadvantage**: Not in the same units as the original data\n",
    "\n",
    "### RMSE (Root Mean Squared Error)\n",
    "- **Formula**: $\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}$\n",
    "- **Characteristics**: Square root of MSE, returns error to original scale\n",
    "- **When to use**: When you want MSE's properties but in interpretable units\n",
    "- **Advantage**: Same units as the target variable\n",
    "\n",
    "### MAE (Mean Absolute Error)\n",
    "- **Formula**: $\\frac{1}{n}\\sum_{i=1}^{n}|y_i - \\hat{y}_i|$\n",
    "- **Characteristics**: Linear penalty for all errors\n",
    "- **When to use**: When all errors should be weighted equally\n",
    "- **Advantage**: More robust to outliers than MSE/RMSE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}